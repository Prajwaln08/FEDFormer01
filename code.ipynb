{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57ab1c3",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44de5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75685294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hr_reading</th>\n",
       "      <th>cal_bur_reading</th>\n",
       "      <th>step_reading</th>\n",
       "      <th>stress_score</th>\n",
       "      <th>stress_grade</th>\n",
       "      <th>spo_saturation</th>\n",
       "      <th>systolic</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>calories</th>\n",
       "      <th>...</th>\n",
       "      <th>is_work_time</th>\n",
       "      <th>is_rest_time</th>\n",
       "      <th>sleep_stress_mean</th>\n",
       "      <th>work_stress_mean</th>\n",
       "      <th>rest_stress_mean</th>\n",
       "      <th>time_since_high_stress_min</th>\n",
       "      <th>spo2_rolling_mean_30min</th>\n",
       "      <th>spo2_variability_30min</th>\n",
       "      <th>spo2_sleep_mean</th>\n",
       "      <th>bgl_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02-13 05:15:00+00:00</td>\n",
       "      <td>59.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>95.5</td>\n",
       "      <td>157.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.549451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.25</td>\n",
       "      <td>1.060660</td>\n",
       "      <td>96.25</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-02-13 05:30:00+00:00</td>\n",
       "      <td>55.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.432234</td>\n",
       "      <td>2.034799</td>\n",
       "      <td>99.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.549451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.25</td>\n",
       "      <td>2.474874</td>\n",
       "      <td>97.25</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-02-13 05:45:00+00:00</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.216117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.00</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>98.00</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-02-13 06:00:00+00:00</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.432234</td>\n",
       "      <td>2.034799</td>\n",
       "      <td>98.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.50</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>97.00</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-02-13 06:15:00+00:00</td>\n",
       "      <td>60.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>96.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.00</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.00</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  hr_reading  cal_bur_reading  step_reading  \\\n",
       "0  2025-02-13 05:15:00+00:00   59.250000              0.0           0.0   \n",
       "1  2025-02-13 05:30:00+00:00   55.500000              0.0           0.0   \n",
       "2  2025-02-13 05:45:00+00:00   63.500000              0.0           0.0   \n",
       "3  2025-02-13 06:00:00+00:00   61.000000              0.0           0.0   \n",
       "4  2025-02-13 06:15:00+00:00   60.666667              0.0           0.0   \n",
       "\n",
       "   stress_score  stress_grade  spo_saturation  systolic  diastolic  calories  \\\n",
       "0     33.000000      2.000000            95.5     157.0       82.0       0.0   \n",
       "1     45.432234      2.034799            99.0     157.0       82.0       0.0   \n",
       "2     40.000000      2.000000            97.0     157.0       82.0       0.0   \n",
       "3     45.432234      2.034799            98.0     157.0       82.0       0.0   \n",
       "4     37.000000      2.000000            96.0     157.0       82.0       0.0   \n",
       "\n",
       "   ...  is_work_time  is_rest_time  sleep_stress_mean  work_stress_mean  \\\n",
       "0  ...             0             0          42.549451               0.0   \n",
       "1  ...             0             0          42.549451               0.0   \n",
       "2  ...             0             0          40.216117               0.0   \n",
       "3  ...             0             1           0.000000               0.0   \n",
       "4  ...             0             1           0.000000               0.0   \n",
       "\n",
       "   rest_stress_mean  time_since_high_stress_min  spo2_rolling_mean_30min  \\\n",
       "0               0.0                         0.0                    96.25   \n",
       "1               0.0                         0.0                    97.25   \n",
       "2               0.0                         0.0                    98.00   \n",
       "3               0.0                         0.0                    97.50   \n",
       "4               0.0                         0.0                    97.00   \n",
       "\n",
       "   spo2_variability_30min  spo2_sleep_mean  bgl_reading  \n",
       "0                1.060660            96.25         86.0  \n",
       "1                2.474874            97.25         87.0  \n",
       "2                1.414214            98.00         91.0  \n",
       "3                0.707107            97.00         92.0  \n",
       "4                1.414214             0.00         88.0  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('CleanedDataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0537a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure timestamp is in datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Sort by timestamp just in case\n",
    "df = df.sort_values('date')\n",
    "df = df.rename(columns={'date': 'timestamp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642ace50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with low variation (<5% unique values): (8 total)\n",
      "- calories\n",
      "- carbs\n",
      "- fat\n",
      "- protein\n",
      "- fiber (g)\n",
      "- GI\n",
      "- hr_intensity_zone\n",
      "- step_intensity_zone\n",
      "Constant Features (no variation): (0 total)\n",
      "Dropped 0 constant features.\n",
      "Remaining shape: (2709, 66) (rows, columns)\n",
      "Columns with missing values (0 total):\n"
     ]
    }
   ],
   "source": [
    "low_variation_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() > 1:\n",
    "        top_freq_ratio = df[col].value_counts(normalize=True).iloc[0]\n",
    "        if top_freq_ratio > 0.95:\n",
    "            low_variation_cols.append(col)\n",
    "\n",
    "print(f\"Columns with low variation (<5% unique values): ({len(low_variation_cols)} total)\")\n",
    "for col in low_variation_cols:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "# Find constant columns (only 1 unique value)\n",
    "constant_features = df.columns[df.nunique() == 1].tolist()\n",
    "\n",
    "print(f\"Constant Features (no variation): ({len(constant_features)} total)\")\n",
    "\n",
    "df_cleaned = df.drop(columns=constant_features)\n",
    "\n",
    "print(f\"Dropped {len(constant_features)} constant features.\")\n",
    "print(f\"Remaining shape: {df_cleaned.shape} (rows, columns)\")\n",
    "\n",
    "# Calculate % of missing values per column\n",
    "missing_percent = df_cleaned.isna().mean() * 100\n",
    "\n",
    "# Filter only columns with missing values\n",
    "missing_percent = missing_percent[missing_percent > 0].sort_values(ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(f\"Columns with missing values ({len(missing_percent)} total):\")\n",
    "for col, pct in missing_percent.items():\n",
    "    print(f\"- {col}: {pct:.2f}% missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47fb0724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns with missing values.\n",
      "Remaining DataFrame shape: (2709, 66) (rows, columns)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where bgl_reading is missing, but keep other columns\n",
    "df_cleaned= df.dropna(axis=1)\n",
    "\n",
    "# Show new shape\n",
    "print(f\"Dropped columns with missing values.\")\n",
    "print(f\"Remaining DataFrame shape: {df_cleaned.shape} (rows, columns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "147a9629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future Regressors:\n",
      "['hr_reading', 'cal_bur_reading', 'step_reading', 'stress_score', 'stress_grade', 'spo_saturation', 'systolic', 'diastolic', 'calories', 'carbs', 'fat', 'protein', 'fiber (g)', 'GI', 'sleep_frag_value', 'duration', 'mean_sleep_score', 'max_sleep_score', 'sleep_score_variability', 'sleep_score_change', 'transition_density', 'sleep_stability_index', 'sleep_stage_volatility', 'cumulative_sleep_score', 'weighted_sleep_depth', 'transition_density_function', 'hr_30min_avg', 'hrv', 'hr_ema', 'hr_zscore', 'hr_baseline_diff', 'hr_trend_slope', 'resting_hr', 'hr_intensity_zone', 'hr_recovery', 'hr_response_delay', 'cal_bur_30min_sum', 'cumulative_cal_burn', 'cal_bur_change_rate', 'cal_bur_activity_intensity', 'cal_bur_time_since_high_intensity', 'steps_30min_sum', 'steps_1hr_sum', 'step_std_30min', 'steps_vs_distance_walked', 'step_rate', 'time_since_last_steps_in_sec', 'steps_active_bout_duration', 'step_intensity_zone', 'stress_3readings_mean', 'stress_6readings_mean', 'stress_var_3readings', 'daily_stress_cumulative', 'hour', 'is_sleep_time', 'is_work_time', 'is_rest_time', 'sleep_stress_mean', 'work_stress_mean', 'rest_stress_mean', 'time_since_high_stress_min', 'spo2_rolling_mean_30min', 'spo2_variability_30min', 'spo2_sleep_mean', 'time_diff']\n",
      "Non-numeric regressors:\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Rename columns for NeuralProphet\n",
    "df_np = df_cleaned.rename(columns={\n",
    "    'timestamp': 'ds',\n",
    "    'bgl_reading': 'y'\n",
    "})\n",
    "\n",
    "# Step 2: Calculate time difference in minutes (actual time gap between rows)\n",
    "df_np['time_diff'] = df_np['ds'].diff().dt.total_seconds() / 60  # convert to minutes\n",
    "df_np['time_diff'].fillna(15, inplace=True)  # Fill first row with default 15 min\n",
    "\n",
    "# Step 3: Define future regressors (all columns except 'ds' and 'y')\n",
    "future_regressors = [col for col in df_np.columns if col not in ['ds', 'y']]\n",
    "print(\"Future Regressors:\")\n",
    "print(future_regressors)\n",
    "\n",
    "non_numeric = df_np[[col for col in df_np.columns if col not in ['ds', 'y']]].select_dtypes(include=['object'])\n",
    "\n",
    "non_numeric.columns = non_numeric.columns.tolist()\n",
    "print(\"Non-numeric regressors:\")\n",
    "print(non_numeric.columns)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply encoding to each column\n",
    "for col in non_numeric.columns:\n",
    "    df_np[col] = label_encoder.fit_transform(df_np[col].astype(str))\n",
    "\n",
    "# Extract useful time components (optional)\n",
    "df_np['weekday'] = df_np['ds'].dt.dayofweek  # 0=Monday\n",
    "df_np['month'] = df_np['ds'].dt.month\n",
    "df_np['day'] = df_np['ds'].dt.day\n",
    "df_np['hour'] = df_np['ds'].dt.hour\n",
    "df_np['minute'] = df_np['ds'].dt.minute\n",
    "df_np['week'] = df_np['ds'].dt.isocalendar().week\n",
    "\n",
    "# Derived features\n",
    "df_np['is_weekend'] = df_np['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Periodic encodings\n",
    "df_np['hour_sin'] = np.sin(2 * np.pi * df_np['hour'] / 24)\n",
    "df_np['hour_cos'] = np.cos(2 * np.pi * df_np['hour'] / 24)\n",
    "df_np['minute_sin'] = np.sin(2 * np.pi * df_np['minute'] / 60)\n",
    "df_np['minute_cos'] = np.cos(2 * np.pi * df_np['minute'] / 60)\n",
    "\n",
    "# Drop original date column\n",
    "if 'date' in df_np.columns:\n",
    "    df_np.drop(columns=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9e1891",
   "metadata": {},
   "source": [
    "Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fa30829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_np.copy()\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "sorted_days = sorted(df['ds'].dt.date.unique())\n",
    "n_days = len(sorted_days)\n",
    "n_train = int(n_days * 0.6)\n",
    "n_val = int(n_days * 0.2)\n",
    "train_days = sorted_days[:n_train]\n",
    "val_days = sorted_days[n_train:n_train + n_val]\n",
    "test_days = sorted_days[n_train + n_val:]\n",
    "df_train = df[df['ds'].dt.date.isin(train_days)].sort_values(by='ds').reset_index(drop=True)\n",
    "df_val = df[df['ds'].dt.date.isin(val_days)].sort_values(by='ds').reset_index(drop=True)\n",
    "df_test = df[df['ds'].dt.date.isin(test_days)].sort_values(by='ds').reset_index(drop=True)\n",
    "\n",
    "future_regressors = [col for col in df_train.columns if col not in ['ds', 'y']]\n",
    "scaler = StandardScaler()\n",
    "df_train[future_regressors] = scaler.fit_transform(df_train[future_regressors])\n",
    "df_val[future_regressors] = scaler.transform(df_val[future_regressors])\n",
    "df_test[future_regressors] = scaler.transform(df_test[future_regressors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb1b462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train days: 17 | Rows: 1611 | Date Range: 2025-02-13 â†’ 2025-03-01\n",
      " Validation days: 5 | Rows: 480 | Date Range: 2025-03-02 â†’ 2025-03-06\n",
      " Test days: 7 | Rows: 618 | Date Range: 2025-03-07 â†’ 2025-03-13\n"
     ]
    }
   ],
   "source": [
    "print(f\" Train days: {len(train_days)} | Rows: {len(df_train)} | Date Range: {df_train['ds'].min().date()} â†’ {df_train['ds'].max().date()}\")\n",
    "print(f\" Validation days: {len(val_days)} | Rows: {len(df_val)} | Date Range: {df_val['ds'].min().date()} â†’ {df_val['ds'].max().date()}\")\n",
    "print(f\" Test days: {len(test_days)} | Rows: {len(df_test)} | Date Range: {df_test['ds'].min().date()} â†’ {df_test['ds'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e28f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val.to_csv('orginal_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee0961e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ds', 'hr_reading', 'cal_bur_reading', 'step_reading', 'stress_score', 'stress_grade', 'spo_saturation', 'systolic', 'diastolic', 'calories', 'carbs', 'fat', 'protein', 'fiber (g)', 'GI', 'sleep_frag_value', 'duration', 'mean_sleep_score', 'max_sleep_score', 'sleep_score_variability', 'sleep_score_change', 'transition_density', 'sleep_stability_index', 'sleep_stage_volatility', 'cumulative_sleep_score', 'weighted_sleep_depth', 'transition_density_function', 'hr_30min_avg', 'hrv', 'hr_ema', 'hr_zscore', 'hr_baseline_diff', 'hr_trend_slope', 'resting_hr', 'hr_intensity_zone', 'hr_recovery', 'hr_response_delay', 'cal_bur_30min_sum', 'cumulative_cal_burn', 'cal_bur_change_rate', 'cal_bur_activity_intensity', 'cal_bur_time_since_high_intensity', 'steps_30min_sum', 'steps_1hr_sum', 'step_std_30min', 'steps_vs_distance_walked', 'step_rate', 'time_since_last_steps_in_sec', 'steps_active_bout_duration', 'step_intensity_zone', 'stress_3readings_mean', 'stress_6readings_mean', 'stress_var_3readings', 'daily_stress_cumulative', 'hour', 'is_sleep_time', 'is_work_time', 'is_rest_time', 'sleep_stress_mean', 'work_stress_mean', 'rest_stress_mean', 'time_since_high_stress_min', 'spo2_rolling_mean_30min', 'spo2_variability_30min', 'spo2_sleep_mean', 'y', 'time_diff', 'weekday', 'month', 'day', 'minute', 'week', 'is_weekend', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos']\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51184e",
   "metadata": {},
   "source": [
    "Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea8f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # General\n",
    "    model = 'FEDformer'\n",
    "    version = 'Fourier'\n",
    "    mode_select = 'random'\n",
    "    modes = 64\n",
    "    L = 3\n",
    "    base = 'legendre'\n",
    "    cross_activation = 'tanh'\n",
    "\n",
    "    # Data\n",
    "    data = 'CleanedDataset'  # or your custom dataset\n",
    "    root_path = './'  # change this if needed\n",
    "    data_path = 'CleanedDataset.csv'  # update if using a different file\n",
    "    features = 'MS'  # multivariate to single\n",
    "    target = 'y'  # output target column\n",
    "    freq = 't'\n",
    "    detail_freq = '15min'\n",
    "    checkpoints = './checkpoints/'\n",
    "\n",
    "    # Forecasting task\n",
    "    seq_len = 12       # input sequence length (past)\n",
    "    label_len = 6      # portion of decoder input (can be 50% of seq_len)\n",
    "    pred_len = 4       # how many future steps to predict\n",
    "\n",
    "    # Model dimensions\n",
    "    enc_in = None    # number of input features (match your dataset)\n",
    "    dec_in = None\n",
    "    c_out = 1          # output is univariate (1 feature)\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    e_layers = 2\n",
    "    d_layers = 1\n",
    "    d_ff = 2048\n",
    "    moving_avg = [12]\n",
    "    factor = 1\n",
    "    distil = True\n",
    "    dropout = 0.05\n",
    "    embed = 'timeF'\n",
    "    activation = 'gelu'\n",
    "    output_attention = False\n",
    "    do_predict = True\n",
    "\n",
    "    # Optimization\n",
    "    num_workers = 2\n",
    "    itr = 1\n",
    "    train_epochs = 50   # use more for real training\n",
    "    batch_size = 16\n",
    "    patience = 2\n",
    "    learning_rate = 0.0001\n",
    "    des = 'fedformer_ms_forecast'\n",
    "    loss = 'mse'\n",
    "    lradj = 'type1'\n",
    "    use_amp = False\n",
    "\n",
    "    # GPU\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    gpu = 0\n",
    "    use_multi_gpu = False\n",
    "    devices = '0'\n",
    "    device_ids = [0]\n",
    "\n",
    "args = Args()\n",
    "total_input_features = len(future_regressors) + 1\n",
    "args.enc_in = total_input_features\n",
    "args.dec_in = total_input_features\n",
    "args.c_out = 1  # ensure we only predict y\n",
    "\n",
    "# Optional device setup logic (multi-GPU safety)\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ', '')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a3fd556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verifying Configuration ---\n",
      "features: MS\n",
      "target: y\n",
      "c_out: 1\n",
      "All configuration checks passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration Verification Block ---\n",
    "print(\"\\n--- Verifying Configuration ---\")\n",
    "print(\"features:\", args.features)\n",
    "print(\"target:\", args.target)\n",
    "print(\"c_out:\", args.c_out)\n",
    "\n",
    "assert args.features == 'MS', \"args.features should be 'MS' for multivariate-to-single prediction\"\n",
    "assert args.target == 'y', \"args.target should be set to the name of the target column, e.g., 'y'\"\n",
    "assert args.c_out == 1, \"args.c_out should be 1 to predict a single target variable\"\n",
    "\n",
    "print(\"All configuration checks passed!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8288105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class MyTSDataset(Dataset):\n",
    "    def __init__(self, df, seq_len=12, label_len=6, pred_len=4, target='y', features='MS', scale=True):\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.pred_len = pred_len\n",
    "        self.target = target\n",
    "        self.features = features\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        df = df.copy()\n",
    "\n",
    "        # Reorder columns: date, features..., target\n",
    "        cols = list(df.columns)\n",
    "        cols.remove('ds')\n",
    "        cols.remove(target)\n",
    "        feature_cols = cols\n",
    "        df = df[['ds'] + feature_cols + [target]]\n",
    "\n",
    "        # Use precomputed time features instead of recomputing\n",
    "        self.data_stamp = df[['weekday', 'month', 'day', 'hour']].values\n",
    "\n",
    "        if features in ['M', 'MS']:\n",
    "            self.encoder_data = df[feature_cols + [target]].values\n",
    "            self.decoder_data = df[[target]].values  # decoder gets only 'y'\n",
    "        else:\n",
    "            self.encoder_data = df[[target]].values\n",
    "            self.decoder_data = df[[target]].values\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.encoder_data[s_begin:s_end]\n",
    "        seq_y = self.decoder_data[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder_data) - self.seq_len - self.pred_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82e9264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DataLoader Setup\n",
    "train_dataset = MyTSDataset(df_train, args.seq_len, args.label_len, args.pred_len, args.target, args.features)\n",
    "val_dataset = MyTSDataset(df_val, args.seq_len, args.label_len, args.pred_len, args.target, args.features)\n",
    "test_dataset = MyTSDataset(df_test, args.seq_len, args.label_len, args.pred_len, args.target, args.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db585e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32fe0509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 1596\n",
      "Validation dataset length: 465\n",
      "Test dataset length: 603\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset length: {len(val_dataset)}\")\n",
    "print(f\"Test dataset length: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd0bbdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input encoder shape: torch.Size([16, 12, 76])\n",
      "Input decoder shape: torch.Size([16, 10, 1])\n",
      "Input feature columns used: ['y', 'hr_reading', 'cal_bur_reading', 'step_reading', 'stress_score', 'stress_grade', 'spo_saturation', 'systolic', 'diastolic', 'calories', 'carbs', 'fat', 'protein', 'fiber (g)', 'GI', 'sleep_frag_value', 'duration', 'mean_sleep_score', 'max_sleep_score', 'sleep_score_variability', 'sleep_score_change', 'transition_density', 'sleep_stability_index', 'sleep_stage_volatility', 'cumulative_sleep_score', 'weighted_sleep_depth', 'transition_density_function', 'hr_30min_avg', 'hrv', 'hr_ema', 'hr_zscore', 'hr_baseline_diff', 'hr_trend_slope', 'resting_hr', 'hr_intensity_zone', 'hr_recovery', 'hr_response_delay', 'cal_bur_30min_sum', 'cumulative_cal_burn', 'cal_bur_change_rate', 'cal_bur_activity_intensity', 'cal_bur_time_since_high_intensity', 'steps_30min_sum', 'steps_1hr_sum', 'step_std_30min', 'steps_vs_distance_walked', 'step_rate', 'time_since_last_steps_in_sec', 'steps_active_bout_duration', 'step_intensity_zone', 'stress_3readings_mean', 'stress_6readings_mean', 'stress_var_3readings', 'daily_stress_cumulative', 'hour', 'is_sleep_time', 'is_work_time', 'is_rest_time', 'sleep_stress_mean', 'work_stress_mean', 'rest_stress_mean', 'time_since_high_stress_min', 'spo2_rolling_mean_30min', 'spo2_variability_30min', 'spo2_sleep_mean', 'time_diff', 'weekday', 'month', 'day', 'minute', 'week', 'is_weekend', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos']\n"
     ]
    }
   ],
   "source": [
    "# Peek one batch to inspect the columns/features\n",
    "sample_batch = next(iter(train_loader))\n",
    "x_enc, x_dec, x_mark_enc, x_mark_dec = sample_batch\n",
    "\n",
    "print(f\"Input encoder shape: {x_enc.shape}\")  # [batch_size, seq_len, num_features]\n",
    "print(f\"Input decoder shape: {x_dec.shape}\")  # [batch_size, label_len + pred_len, num_features]\n",
    "\n",
    "# Corresponding feature names\n",
    "input_feature_names = ['y'] + future_regressors\n",
    "print(f\"Input feature columns used: {input_feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "541d0d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_enc shape: torch.Size([16, 12, 76]), expected features: 76\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_enc shape: {x_enc.shape}, expected features: {args.enc_in}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3153599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layers.Embed import DataEmbedding, DataEmbedding_wo_pos\n",
    "from layers.AutoCorrelation import AutoCorrelation, AutoCorrelationLayer\n",
    "from layers.FourierCorrelation import FourierBlock, FourierCrossAttention\n",
    "from layers.MultiWaveletCorrelation import MultiWaveletCross, MultiWaveletTransform\n",
    "from layers.SelfAttention_Family import FullAttention, ProbAttention\n",
    "from layers.Autoformer_EncDec import Encoder, Decoder, EncoderLayer, DecoderLayer, my_Layernorm, series_decomp, series_decomp_multi\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if args.use_gpu else \"cpu\")\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    FEDformer performs the attention mechanism on frequency domain and achieved O(N) complexity\n",
    "    \"\"\"\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.version = configs.version\n",
    "        self.mode_select = configs.mode_select\n",
    "        self.modes = configs.modes\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.label_len = configs.label_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.output_attention = configs.output_attention\n",
    "        self.c_out = configs.c_out\n",
    "\n",
    "        # Decomp\n",
    "        kernel_size = configs.moving_avg\n",
    "        if isinstance(kernel_size, list):\n",
    "            self.decomp = series_decomp_multi(kernel_size)\n",
    "        else:\n",
    "            self.decomp = series_decomp(kernel_size)\n",
    "\n",
    "        # Embedding\n",
    "        # The series-wise connection inherently contains the sequential information.\n",
    "        # Thus, we can discard the position embedding of transformers.\n",
    "        self.enc_embedding = DataEmbedding_wo_pos(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                                  configs.dropout)\n",
    "        self.dec_embedding = DataEmbedding_wo_pos(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                                  configs.dropout)\n",
    "\n",
    "        if configs.version == 'Wavelets':\n",
    "            encoder_self_att = MultiWaveletTransform(ich=configs.d_model, L=configs.L, base=configs.base)\n",
    "            decoder_self_att = MultiWaveletTransform(ich=configs.d_model, L=configs.L, base=configs.base)\n",
    "            decoder_cross_att = MultiWaveletCross(in_channels=configs.d_model,\n",
    "                                                  out_channels=configs.d_model,\n",
    "                                                  seq_len_q=self.seq_len // 2 + self.pred_len,\n",
    "                                                  seq_len_kv=self.seq_len,\n",
    "                                                  modes=configs.modes,\n",
    "                                                  ich=configs.d_model,\n",
    "                                                  base=configs.base,\n",
    "                                                  activation=configs.cross_activation)\n",
    "        else:\n",
    "            encoder_self_att = FourierBlock(in_channels=configs.d_model,\n",
    "                                            out_channels=configs.d_model,\n",
    "                                            seq_len=self.seq_len,\n",
    "                                            modes=configs.modes,\n",
    "                                            mode_select_method=configs.mode_select)\n",
    "            decoder_self_att = FourierBlock(in_channels=configs.d_model,\n",
    "                                            out_channels=configs.d_model,\n",
    "                                            seq_len=self.seq_len//2+self.pred_len,\n",
    "                                            modes=configs.modes,\n",
    "                                            mode_select_method=configs.mode_select)\n",
    "            decoder_cross_att = FourierCrossAttention(in_channels=configs.d_model,\n",
    "                                                      out_channels=configs.d_model,\n",
    "                                                      seq_len_q=self.seq_len//2+self.pred_len,\n",
    "                                                      seq_len_kv=self.seq_len,\n",
    "                                                      modes=configs.modes,\n",
    "                                                      mode_select_method=configs.mode_select)\n",
    "        # Encoder\n",
    "        enc_modes = int(min(configs.modes, configs.seq_len//2))\n",
    "        dec_modes = int(min(configs.modes, (configs.seq_len//2+configs.pred_len)//2))\n",
    "        print('enc_modes: {}, dec_modes: {}'.format(enc_modes, dec_modes))\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        encoder_self_att,\n",
    "                        configs.d_model, configs.n_heads),\n",
    "\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    moving_avg=configs.moving_avg,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation\n",
    "                ) for l in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=my_Layernorm(configs.d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        decoder_self_att,\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    AutoCorrelationLayer(\n",
    "                        decoder_cross_att,\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    configs.d_model,\n",
    "                    configs.c_out,\n",
    "                    configs.d_ff,\n",
    "                    moving_avg=configs.moving_avg,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                )\n",
    "                for l in range(configs.d_layers)\n",
    "            ],\n",
    "            norm_layer=my_Layernorm(configs.d_model),\n",
    "            projection=nn.Linear(configs.d_model, configs.c_out, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "        # decomp init\n",
    "        mean = torch.mean(x_enc, dim=1).unsqueeze(1).repeat(1, self.pred_len, 1)\n",
    "        zeros = torch.zeros([x_dec.shape[0], self.pred_len, x_dec.shape[2]]).to(device)  # cuda()\n",
    "        seasonal_init, trend_init = self.decomp(x_enc)\n",
    "        # decoder input\n",
    "        trend_init = torch.cat([trend_init[:, -self.label_len:, :], mean], dim=1)\n",
    "        seasonal_init = F.pad(seasonal_init[:, -self.label_len:, :], (0, 0, 0, self.pred_len))\n",
    "        # enc\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "        # dec\n",
    "        dec_out = self.dec_embedding(seasonal_init, x_mark_dec)\n",
    "        seasonal_part, trend_part = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask,\n",
    "                                                 trend=trend_init)\n",
    "        #final\n",
    "        dec_out = trend_part + seasonal_part\n",
    "        dec_out = dec_out[:, -self.pred_len:, :self.c_out]  # restrict to just target\n",
    "\n",
    "        if self.output_attention:\n",
    "            return dec_out, attns\n",
    "        else:\n",
    "            return dec_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02e9a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fourier enhanced block used!\n",
      "modes=64, index=[0, 1, 2, 3, 4, 5]\n",
      "fourier enhanced block used!\n",
      "modes=64, index=[0, 1, 2, 3, 4]\n",
      " fourier enhanced cross attention used!\n",
      "modes_q=5, index_q=[0, 1, 2, 3, 4]\n",
      "modes_kv=6, index_kv=[0, 1, 2, 3, 4, 5]\n",
      "enc_modes: 6, dec_modes: 5\n",
      "\n",
      "Forward pass output shape: torch.Size([16, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# 3. Forward Pass Example\n",
    "batch = next(iter(train_loader))\n",
    "x_enc, x_dec, x_mark_enc, x_mark_dec = [x.float().to(device) for x in batch]\n",
    "\n",
    "model = Model(args).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "\n",
    "print(\"\\nForward pass output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6213576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training, Validation, and Test Evaluation Loop\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2b4aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x_enc, x_dec, x_mark_enc, x_mark_dec = [x.float().to(device) for x in batch]\n",
    "            outputs = model(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "            preds.append(outputs.cpu())\n",
    "            trues.append(x_dec[:, -args.pred_len:, :args.c_out].cpu())\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    trues = torch.cat(trues, dim=0).numpy()\n",
    "    return preds, trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "884e5c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | Train Loss: 4470.3183 | Val Loss: 3421.3914\n",
      "Epoch [2/50] | Train Loss: 1684.8055 | Val Loss: 1801.5884\n",
      "Epoch [3/50] | Train Loss: 626.7701 | Val Loss: 642.0796\n",
      "Epoch [4/50] | Train Loss: 291.1423 | Val Loss: 519.0450\n",
      "Epoch [5/50] | Train Loss: 208.2501 | Val Loss: 362.0822\n",
      "Epoch [6/50] | Train Loss: 183.5691 | Val Loss: 328.0480\n",
      "Epoch [7/50] | Train Loss: 174.5122 | Val Loss: 312.7216\n",
      "Epoch [8/50] | Train Loss: 159.2595 | Val Loss: 289.2917\n",
      "Epoch [9/50] | Train Loss: 156.6200 | Val Loss: 235.5773\n",
      "Epoch [10/50] | Train Loss: 150.0407 | Val Loss: 254.8682\n",
      "Epoch [11/50] | Train Loss: 144.3252 | Val Loss: 236.2497\n",
      "Epoch [12/50] | Train Loss: 143.2649 | Val Loss: 251.3521\n",
      "Epoch [13/50] | Train Loss: 139.5111 | Val Loss: 222.8016\n",
      "Epoch [14/50] | Train Loss: 133.8525 | Val Loss: 220.6507\n",
      "Epoch [15/50] | Train Loss: 133.2353 | Val Loss: 236.4669\n",
      "Epoch [16/50] | Train Loss: 128.5672 | Val Loss: 240.8124\n",
      "Epoch [17/50] | Train Loss: 123.7307 | Val Loss: 231.9959\n",
      "Epoch [18/50] | Train Loss: 126.7883 | Val Loss: 270.1440\n",
      "Epoch [19/50] | Train Loss: 122.2012 | Val Loss: 232.8207\n",
      "Epoch [20/50] | Train Loss: 121.5163 | Val Loss: 244.1726\n",
      "Epoch [21/50] | Train Loss: 119.8982 | Val Loss: 293.8945\n",
      "Epoch [22/50] | Train Loss: 118.8361 | Val Loss: 261.9957\n",
      "Epoch [23/50] | Train Loss: 116.5805 | Val Loss: 258.7751\n",
      "Epoch [24/50] | Train Loss: 110.0306 | Val Loss: 256.0818\n",
      "Epoch [25/50] | Train Loss: 108.9868 | Val Loss: 281.4174\n",
      "Epoch [26/50] | Train Loss: 114.1139 | Val Loss: 263.1417\n",
      "Epoch [27/50] | Train Loss: 106.3025 | Val Loss: 284.7551\n",
      "Epoch [28/50] | Train Loss: 105.5841 | Val Loss: 301.1628\n",
      "Epoch [29/50] | Train Loss: 104.6797 | Val Loss: 289.3392\n",
      "Epoch [30/50] | Train Loss: 100.2387 | Val Loss: 311.7826\n",
      "Epoch [31/50] | Train Loss: 96.0181 | Val Loss: 356.1053\n",
      "Epoch [32/50] | Train Loss: 95.6546 | Val Loss: 383.5560\n",
      "Epoch [33/50] | Train Loss: 91.3883 | Val Loss: 281.6135\n",
      "Epoch [34/50] | Train Loss: 96.7137 | Val Loss: 390.4728\n",
      "Epoch [35/50] | Train Loss: 88.8502 | Val Loss: 282.6799\n",
      "Epoch [36/50] | Train Loss: 87.0488 | Val Loss: 289.5791\n",
      "Epoch [37/50] | Train Loss: 84.9743 | Val Loss: 286.8466\n",
      "Epoch [38/50] | Train Loss: 81.1267 | Val Loss: 308.2785\n",
      "Epoch [39/50] | Train Loss: 81.1711 | Val Loss: 315.6619\n",
      "Epoch [40/50] | Train Loss: 78.9064 | Val Loss: 400.3171\n",
      "Epoch [41/50] | Train Loss: 78.9628 | Val Loss: 353.4672\n",
      "Epoch [42/50] | Train Loss: 76.3082 | Val Loss: 304.7283\n",
      "Epoch [43/50] | Train Loss: 76.2248 | Val Loss: 321.0155\n",
      "Epoch [44/50] | Train Loss: 73.9375 | Val Loss: 325.8885\n",
      "Epoch [45/50] | Train Loss: 70.1939 | Val Loss: 359.8841\n",
      "Epoch [46/50] | Train Loss: 66.8670 | Val Loss: 358.5316\n",
      "Epoch [47/50] | Train Loss: 66.5774 | Val Loss: 380.9355\n",
      "Epoch [48/50] | Train Loss: 67.5418 | Val Loss: 330.0584\n",
      "Epoch [49/50] | Train Loss: 65.4017 | Val Loss: 371.1807\n",
      "Epoch [50/50] | Train Loss: 61.4783 | Val Loss: 362.4617\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.train_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x_enc, x_dec, x_mark_enc, x_mark_dec = [x.float().to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "        loss = criterion(outputs, x_dec[:, -args.pred_len:, :args.c_out])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    val_preds, val_trues = evaluate(val_loader)\n",
    "    val_loss = criterion(torch.tensor(val_preds), torch.tensor(val_trues)).item()\n",
    "    print(f\"Epoch [{epoch+1}/{args.train_epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e16bff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 384.2806\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "preds, trues = evaluate(test_loader)\n",
    "test_loss = criterion(torch.tensor(preds), torch.tensor(trues)).item()\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape predictions from [B, pred_len, c_out] â†’ [B * pred_len, c_out]\n",
    "val_preds_2d = val_preds.reshape(-1, val_preds.shape[-1])\n",
    "test_preds_2d = preds.reshape(-1, preds.shape[-1])\n",
    "\n",
    "# Align timestamps accordingly\n",
    "val_timestamps = df_val['ds'].iloc[args.seq_len + args.label_len - 1:].reset_index(drop=True)\n",
    "val_timestamps = val_timestamps.iloc[:len(val_preds_2d)]\n",
    "\n",
    "test_timestamps = df_test['ds'].iloc[args.seq_len + args.label_len - 1:].reset_index(drop=True)\n",
    "test_timestamps = test_timestamps.iloc[:len(test_preds_2d)]\n",
    "\n",
    "# Create DataFrames\n",
    "df_val_output = pd.DataFrame(val_preds_2d, columns=[f\"pred_{i}\" for i in range(val_preds_2d.shape[-1])])\n",
    "df_val_output['ds'] = val_timestamps\n",
    "\n",
    "df_test_output = pd.DataFrame(test_preds_2d, columns=[f\"pred_{i}\" for i in range(test_preds_2d.shape[-1])])\n",
    "df_test_output['ds'] = test_timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "718513c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first few rows of validation predictions\n",
    "df_val_output.to_csv('output_val.csv')\n",
    "# df_val_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26524fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_0</th>\n",
       "      <th>ds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95.579010</td>\n",
       "      <td>2025-03-07 04:15:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.953041</td>\n",
       "      <td>2025-03-07 04:30:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.331367</td>\n",
       "      <td>2025-03-07 04:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.199821</td>\n",
       "      <td>2025-03-07 05:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.179825</td>\n",
       "      <td>2025-03-07 05:15:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred_0                        ds\n",
       "0   95.579010 2025-03-07 04:15:00+00:00\n",
       "1   95.953041 2025-03-07 04:30:00+00:00\n",
       "2  104.331367 2025-03-07 04:45:00+00:00\n",
       "3   98.199821 2025-03-07 05:00:00+00:00\n",
       "4  100.179825 2025-03-07 05:15:00+00:00"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first few rows of test predictions\n",
    "df_test_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02143c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val_output.to_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d82c87d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "MSE: 362.4617\n",
      "RMSE: 19.0384\n",
      "MAE: 15.4654\n",
      "R2: -0.3753\n",
      "SMAPE: 14.9081\n",
      "\n",
      "Test Metrics:\n",
      "MSE: 384.2807\n",
      "RMSE: 19.6031\n",
      "MAE: 13.8953\n",
      "R2: -0.7367\n",
      "SMAPE: 12.7938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# --- Evaluation Metrics Block ---\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff) * 100\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    smape_val = smape(y_true, y_pred)\n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'SMAPE': smape_val\n",
    "    }\n",
    "\n",
    "# Flatten predictions and truths for evaluation\n",
    "val_preds_flat = val_preds.reshape(-1, val_preds.shape[-1])[:, 0]\n",
    "val_trues_flat = val_trues.reshape(-1, val_trues.shape[-1])[:, 0]\n",
    "test_preds_flat = preds.reshape(-1, preds.shape[-1])[:, 0]\n",
    "test_trues_flat = trues.reshape(-1, trues.shape[-1])[:, 0]\n",
    "\n",
    "val_metrics = evaluate_metrics(val_trues_flat, val_preds_flat)\n",
    "test_metrics = evaluate_metrics(test_trues_flat, test_preds_flat)\n",
    "\n",
    "print(\"Validation Metrics:\")\n",
    "for k, v in val_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29449234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output_val and output_test DataFrames with aligned timestamps\n",
    "\n",
    "# Reshape predictions and truths\n",
    "val_preds_flat = val_preds.reshape(-1)\n",
    "val_trues_flat = val_trues.reshape(-1)\n",
    "test_preds_flat = preds.reshape(-1)\n",
    "test_trues_flat = trues.reshape(-1)\n",
    "\n",
    "# Construct aligned timestamps from prediction windows\n",
    "val_timestamps = []\n",
    "val_limit = len(df_val) - args.label_len - args.seq_len - 1\n",
    "for i in range(min(len(val_preds), val_limit)):\n",
    "    base_idx = i + args.seq_len + args.label_len - 1\n",
    "    if base_idx < len(df_val):\n",
    "        base_time = df_val['ds'].iloc[base_idx]\n",
    "        future_times = pd.date_range(start=base_time, periods=args.pred_len, freq=args.detail_freq)\n",
    "        val_timestamps.extend(future_times)\n",
    "val_timestamps = val_timestamps[:len(val_preds_flat)]\n",
    "\n",
    "test_timestamps = []\n",
    "test_limit = len(df_test) - args.label_len - args.seq_len - 1\n",
    "for i in range(min(len(preds), test_limit)):\n",
    "    base_idx = i + args.seq_len + args.label_len - 1\n",
    "    if base_idx < len(df_test):\n",
    "        base_time = df_test['ds'].iloc[base_idx]\n",
    "        future_times = pd.date_range(start=base_time, periods=args.pred_len, freq=args.detail_freq)\n",
    "        test_timestamps.extend(future_times)\n",
    "test_timestamps = test_timestamps[:len(test_preds_flat)]\n",
    "\n",
    "# Final length alignment check\n",
    "min_val_len = min(len(val_timestamps), len(val_preds_flat), len(val_trues_flat))\n",
    "min_test_len = min(len(test_timestamps), len(test_preds_flat), len(test_trues_flat))\n",
    "\n",
    "# Create final output DataFrames\n",
    "output_val = pd.DataFrame({\n",
    "    'ds': val_timestamps[:min_val_len],\n",
    "    'y_true': val_trues_flat[:min_val_len],\n",
    "    'y_pred': val_preds_flat[:min_val_len]\n",
    "})\n",
    "\n",
    "output_test = pd.DataFrame({\n",
    "    'ds': test_timestamps[:min_test_len],\n",
    "    'y_true': test_trues_flat[:min_test_len],\n",
    "    'y_pred': test_preds_flat[:min_test_len]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1238c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-02 04:15:00+00:00</td>\n",
       "      <td>80.666664</td>\n",
       "      <td>110.321518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-02 04:30:00+00:00</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>96.385452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-02 04:45:00+00:00</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>94.864960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-02 05:00:00+00:00</td>\n",
       "      <td>81.666664</td>\n",
       "      <td>93.977966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-02 04:30:00+00:00</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>100.416634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>2025-03-06 23:45:00+00:00</td>\n",
       "      <td>108.666664</td>\n",
       "      <td>103.267921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1840</th>\n",
       "      <td>2025-03-06 23:15:00+00:00</td>\n",
       "      <td>119.333336</td>\n",
       "      <td>98.398338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>2025-03-06 23:30:00+00:00</td>\n",
       "      <td>116.666664</td>\n",
       "      <td>98.177521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>2025-03-06 23:45:00+00:00</td>\n",
       "      <td>108.666664</td>\n",
       "      <td>102.819344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>2025-03-07 00:00:00+00:00</td>\n",
       "      <td>103.666664</td>\n",
       "      <td>97.946533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1844 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ds      y_true      y_pred\n",
       "0    2025-03-02 04:15:00+00:00   80.666664  110.321518\n",
       "1    2025-03-02 04:30:00+00:00   80.000000   96.385452\n",
       "2    2025-03-02 04:45:00+00:00   82.000000   94.864960\n",
       "3    2025-03-02 05:00:00+00:00   81.666664   93.977966\n",
       "4    2025-03-02 04:30:00+00:00   80.000000  100.416634\n",
       "...                        ...         ...         ...\n",
       "1839 2025-03-06 23:45:00+00:00  108.666664  103.267921\n",
       "1840 2025-03-06 23:15:00+00:00  119.333336   98.398338\n",
       "1841 2025-03-06 23:30:00+00:00  116.666664   98.177521\n",
       "1842 2025-03-06 23:45:00+00:00  108.666664  102.819344\n",
       "1843 2025-03-07 00:00:00+00:00  103.666664   97.946533\n",
       "\n",
       "[1844 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59411544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-07 04:15:00+00:00</td>\n",
       "      <td>103.333336</td>\n",
       "      <td>95.579010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-07 04:30:00+00:00</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>95.953041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-07 04:45:00+00:00</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.331367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-07 05:00:00+00:00</td>\n",
       "      <td>102.666664</td>\n",
       "      <td>98.199821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-07 04:30:00+00:00</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>100.179825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>2025-03-13 10:45:00+00:00</td>\n",
       "      <td>123.666664</td>\n",
       "      <td>97.144440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>2025-03-13 10:15:00+00:00</td>\n",
       "      <td>111.666664</td>\n",
       "      <td>98.079048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>2025-03-13 10:30:00+00:00</td>\n",
       "      <td>122.333336</td>\n",
       "      <td>106.433426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>2025-03-13 10:45:00+00:00</td>\n",
       "      <td>123.666664</td>\n",
       "      <td>102.655563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>2025-03-13 11:00:00+00:00</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>102.765747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2368 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ds      y_true      y_pred\n",
       "0    2025-03-07 04:15:00+00:00  103.333336   95.579010\n",
       "1    2025-03-07 04:30:00+00:00  103.000000   95.953041\n",
       "2    2025-03-07 04:45:00+00:00  104.000000  104.331367\n",
       "3    2025-03-07 05:00:00+00:00  102.666664   98.199821\n",
       "4    2025-03-07 04:30:00+00:00  103.000000  100.179825\n",
       "...                        ...         ...         ...\n",
       "2363 2025-03-13 10:45:00+00:00  123.666664   97.144440\n",
       "2364 2025-03-13 10:15:00+00:00  111.666664   98.079048\n",
       "2365 2025-03-13 10:30:00+00:00  122.333336  106.433426\n",
       "2366 2025-03-13 10:45:00+00:00  123.666664  102.655563\n",
       "2367 2025-03-13 11:00:00+00:00  114.000000  102.765747\n",
       "\n",
       "[2368 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0babf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
